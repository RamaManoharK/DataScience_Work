{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfe7ac7b-cc8b-42d8-9f2b-f75a2d5c0af2",
   "metadata": {},
   "source": [
    "### Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data.\n",
    "        \n",
    "        Web scraping refers to the extraction of data from a website. This information is collected and then exported into a format that is more useful for the user. Be it a spreadsheet or an API.\n",
    "        Although web scraping can be done manually, in most cases, automated tools are preferred when scraping web data as they can be less costly and work at a faster rate.\n",
    "        \n",
    "        Examples : \n",
    "        ----------\n",
    "            Real Estate Listing Scraping\n",
    "                Many real estate agents use web scraping to populate their database of available properties for sale or for rent.\n",
    "                \n",
    "            Industry Statistics and Insights\n",
    "                Many companies use web scraping to build massive databases and draw industry-specific insights from these. These companies can then sell access to these insights to companies in said industries.\n",
    "                \n",
    "            Comparison Shopping Sites\n",
    "                Some several websites and applications can help you to easily compare pricing between several retailers for the same product.\n",
    "                \n",
    "            Lead Generation\n",
    "                One incredibly popular use of web scraping is lead generation. This use is so popular in fact, that we have written an entire guide on using web scraping for lead generation.\n",
    "            \n",
    "        Other Examples :\n",
    "        ---------------\n",
    "                Scraping stock prices into an app API\n",
    "                Scraping data from a store locator to create a list of business locations\n",
    "                Scraping product data from sites like Amazon or Flipkart for competitor analysis\n",
    "                Scraping sports stats for betting or fantasy leagues\n",
    "                Scraping site data before a website migration\n",
    "                Scraping product details for comparison shopping\n",
    "                Scraping financial data for market research and insights\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3473f6c9-a5a2-438d-b8d3-18ef9a0d2f2a",
   "metadata": {},
   "source": [
    "### Q2. What are the different methods used for Web Scraping?\n",
    "\n",
    "        The most common techniques used for Web Scraping are:\n",
    "            \n",
    "            Human Copy-and-Paste : \n",
    "                Manually copying and pasting data from a web page into a text file or spreadsheet is the most basic form of web scraping.\n",
    "            \n",
    "            Text Pattern Matching : \n",
    "                The UNIX grep command or regular expression-matching facilities of programming languages can be used to extract information from web pages in a simple yet powerful way (for instance Perl or Python).\n",
    "            \n",
    "            HTTP Programming : \n",
    "                Static and dynamic web pages can be retrieved by using socket programming to send HTTP requests to a remote web server.\n",
    "                \n",
    "            HTML Parsing : \n",
    "                Many websites contain large collections of pages that are dynamically generated from an underlying structured source, such as a database. A common script or template is typically used to encode data from the same category into similar pages. A wrapper is a program in data mining that detects such templates in a specific information source, extracts its content, and converts it to a relational form.\n",
    "            \n",
    "            DOM Parsing : \n",
    "                More information: Object Model for Documents, Programs can retrieve dynamic content generated by client-side scripts by embedding a full-fledged web browser, such as Internet Explorer or the Mozilla browser control.\n",
    "                \n",
    "            Vertical Aggregation : \n",
    "                Several companies have created vertically specific harvesting platforms. These platforms generate and monitor a plethora of “bots” for specific verticals with no “man in the loop” (direct human involvement) and no work related to a specific target site. \n",
    "                \n",
    "            Semantic Annotation Recognizing :\n",
    "                The scraped pages may include metadata, semantic markups, and annotations that can be used to locate specific data snippets.This technique can be viewed as a subset of DOM parsing if the annotations are embedded in the pages, as Microformat does.\n",
    "                \n",
    "            Computer Vision Web-Page Analysis :\n",
    "                There are efforts using machine learning and computer vision to identify and extract information from web pages by visually interpreting pages as a human would."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd1bae4-bb78-42e8-99f6-9d61c8cfe058",
   "metadata": {},
   "source": [
    "### Q3. What is Beautiful Soup? Why is it used?\n",
    "\n",
    "        Beautiful Soup is a python library that makes it easy to scrape information from web pages. It sits atop an HTML or XML parser, providing Pythonic idioms for iterating, searching, and modifying the parse tree.\n",
    "    \n",
    "        Beautiful Soup provides simple methods for navigating, searching, and modifying a parse tree in HTML, XML files. It transforms a complex HTML document into a tree of Python objects. It also automatically converts the document to Unicode, so we don’t have to think about encodings. This tool not only helps us to scrape but also to clean the data. Beautiful Soup supports the HTML parser included in Python’s standard library, but it also supports several third-party Python parsers like lxml or hml5lib.\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a128b85-7c4d-427e-aaa1-879e9386a1da",
   "metadata": {},
   "source": [
    "### Q4. Why is flask used in this Web Scraping project?\n",
    "    \n",
    "            Flask is a lightweight framework to build web application and it is a microframework for developers, designed to enable them to create and scale web apps quickly and simply.\n",
    "            \n",
    "            using Flask, we can turn our webscraper app into webapp."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4103361-cec7-4ef0-94a4-213855cca10c",
   "metadata": {},
   "source": [
    "### Q5. Write the names of AWS services used in this project. Also, explain the use of each service.\n",
    "\n",
    "    we use two main services of AWS in our webscrapping project:\n",
    "        Elastic Beanstalk\n",
    "        CodePipeline\n",
    "    \n",
    "    Elastic Beanstalk :\n",
    "    ---------------------    \n",
    "        AWS Elastic Beanstalk is an AWS managed service for web applications.Elastic beanstalk is a pre-configured EC2 server that can directly take up our application code and environment configurations and use it to automatically provision and deploy the required resources within AWS to run the web application.\n",
    "        \n",
    "        With AWS Elastic Beanstalk, we can quickly deploy and manage applications in the AWS Cloud without worrying about the infrastructure that runs those applications. AWS Elastic Beanstalk reduces management complexity without restricting choice or control. We simply upload our application, and AWS Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring.\n",
    "    \n",
    "    CodePipiline : \n",
    "    -------------------    \n",
    "            AWS CodePipeline is an Amazon Web Services productand is a continuous delivery service that automates the software deployment process, allowing a developer to quickly model, visualize and deliver code for new features and updates. \n",
    "        \n",
    "        AWS CodePipeline automatically builds, tests and launches an application each time the code is changed. CodePipeline automates the steps required to release our software changes continuously.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff0818d-118b-4955-902f-6b74efa2ed59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
